{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OnlyCNN import TRepNet as only_cnn\n",
    "from OnlyResNet import TRepNet as only_resnet\n",
    "from OnlyWavenet import TRepNet as only_wavenet\n",
    "from CNNandResNet import TRepNet as cnn_res\n",
    "from WavenetandResnet import TRepNet as wave_res\n",
    "from WavenetandCNN import TRepNet as wave_cnn\n",
    "from NoNoise import TRepNet as no_noise\n",
    "from TRepNet import TRepNet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_data, random_benchmark, list_datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from Imputation import remove_and_impute\n",
    "from Models import SAE, CNN_AE, LSTM_AE, GRU_AE, Bi_LSTM_AE, CNN_Bi_LSTM_AE, Causal_CNN_AE, Wavenet, Attention_Bi_LSTM_AE, Attention_CNN_Bi_LSTM_AE, Attention_Wavenet\n",
    "\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def flatten_ts(train, test):\n",
    "    new_train, new_test = [], []\n",
    "    train_lens = []\n",
    "    \n",
    "    for _, row in train.iterrows():\n",
    "        for i in row.index:\n",
    "            train_lens.append(len(row[i]))\n",
    "\n",
    "    maxlen = np.ceil(np.average(train_lens)).astype(int)\n",
    "    \n",
    "    for _, row in train.iterrows():\n",
    "        new_list = []\n",
    "        for i in row.index:\n",
    "            ts = []\n",
    "            for j in range(len(row[i])):\n",
    "                ts.append(row[i][j])\n",
    "            new_list.append(ts)\n",
    "        new_train.append(pad_sequences(new_list, maxlen=maxlen, dtype='float32'))\n",
    "        \n",
    "    for _, row in test.iterrows():\n",
    "        new_list = []\n",
    "        for i in row.index:\n",
    "            ts = []\n",
    "            for j in range(len(row[i])):\n",
    "                ts.append(row[i][j])\n",
    "            new_list.append(ts)\n",
    "        new_test.append(pad_sequences(new_list, maxlen=maxlen, dtype='float32'))\n",
    "            \n",
    "    train_df = pd.DataFrame(np.array(new_train).reshape(train.shape[0], maxlen * train.columns.shape[0]))\n",
    "    test_df = pd.DataFrame(np.array(new_test).reshape(test.shape[0], maxlen * train.columns.shape[0]))\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(train_df)\n",
    "    return scaler.transform(train_df), scaler.transform(test_df), maxlen * train.columns.shape[0]\n",
    "#     return np.array(train_df), np.array(test_df), maxlen * train.columns.shape[0]\n",
    "\n",
    "def rnn_reshape(train, test, n_steps, n_features):\n",
    "#     train, test = flatten_ts(train, test)\n",
    "    return train.reshape(train.shape[0], n_steps, n_features), test.reshape(test.shape[0], n_steps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "# mc = keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  DiatomSizeReduction\n",
      "SVM >> {'accuracy': 0.8856209150326797, 'f1': 0.842923495438463}\n",
      "SVM >> {'accuracy': 0.5686274509803921, 'f1': 0.4339709373485465}\n",
      "SVM >> {'accuracy': 0.9183006535947712, 'f1': 0.9027866068707274}\n",
      "SVM >> {'accuracy': 0.8758169934640523, 'f1': 0.827534194051909}\n",
      "SVM >> {'accuracy': 0.8888888888888888, 'f1': 0.8403307905971613}\n",
      "SVM >> {'accuracy': 0.9084967320261438, 'f1': 0.8868189952676339}\n",
      "SVM >> {'accuracy': 0.8725490196078431, 'f1': 0.8243738086701949}\n",
      "SVM >> {'accuracy': 0.8888888888888888, 'f1': 0.8410942714392521}\n"
     ]
    }
   ],
   "source": [
    "# when tuning start with learning rate->mini_batch_size -> \n",
    "# momentum-> #hidden_units -> # learning_rate_decay -> #layers \n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data_name = 'DiatomSizeReduction'\n",
    "univariate = True\n",
    "print('Data: ', data_name)\n",
    "scores = []\n",
    "for fn in [only_cnn, only_resnet, only_wavenet, cnn_res, wave_res, wave_cnn, no_noise, TRepNet]:\n",
    "    train_x, train_y, test_x, test_y = load_data(data_name, univariate=univariate)    \n",
    "    #     n_steps = train_x.iloc[0][0].shape[0]\n",
    "    n_features = train_x.columns.shape[0]\n",
    "\n",
    "    X_train, X_test, n_steps = flatten_ts(train_x, test_x)\n",
    "    X_train, X_test = rnn_reshape(X_train, X_test, n_steps // n_features, n_features)\n",
    "\n",
    "    encoder, decoder = fn(n_steps // n_features, n_features, activation='elu')\n",
    "    model = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "    model.compile(loss=\"mae\", optimizer=keras.optimizers.Nadam(lr=0.001, clipnorm=1.), metrics=['mae'])\n",
    "    history = model.fit(X_train, X_train, epochs=500, batch_size=16, validation_data=[X_test, X_test], callbacks=[es], verbose=0, shuffle=False)\n",
    "\n",
    "    # Codings\n",
    "    codings_train = encoder.predict(X_train)\n",
    "    codings_test = encoder.predict(X_test)\n",
    "\n",
    "    #     # RF\n",
    "    #     rf_clf.fit(codings_train, train_y)\n",
    "    #     pred = rf_clf.predict(codings_test)\n",
    "    #     rf_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "    #     print('RF >>', rf_scores)\n",
    "\n",
    "    # SVM\n",
    "    svm_clf = SVC(random_state=7, gamma='scale')\n",
    "#     nb_classes = np.unique(train_y).shape[0]\n",
    "#     train_size = codings_train.shape[0]\n",
    "#     if train_size // nb_classes < 5 or train_size < 50:\n",
    "#         svm_clf.fit(codings_train, train_y)\n",
    "#     else:\n",
    "#         grid_search = GridSearchCV(svm_clf, {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, np.inf]}, cv=5, iid=False, n_jobs=-1)\n",
    "#         if train_size <= 10000:\n",
    "#             grid_search.fit(codings_train, train_y)\n",
    "#         else:\n",
    "#             codings_train, _, train_y, _  = train_test_split(codings_train, train_y, train_size=10000, random_state=7, stratify=train_y)\n",
    "#             grid_search.fit(codings_train, train_y)       \n",
    "#         svm_clf = grid_search.best_estimator_\n",
    "\n",
    "#         svm_clf.fit(codings_train, train_y)\n",
    "\n",
    "    svm_clf.fit(codings_train, train_y)\n",
    "    pred = svm_clf.predict(codings_test)\n",
    "\n",
    "    svm_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "    scores.append(svm_scores)\n",
    "    print('SVM >>', svm_scores)\n",
    "pd.DataFrame(scores).to_csv('./results/ablation-'+data_name+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
