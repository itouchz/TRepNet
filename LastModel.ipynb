{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_data, random_benchmark, list_datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from Imputation import remove_and_impute\n",
    "from Models import SAE, CNN_AE, LSTM_AE, GRU_AE, Bi_LSTM_AE, CNN_Bi_LSTM_AE, Causal_CNN_AE, Wavenet, Attention_Bi_LSTM_AE, Attention_CNN_Bi_LSTM_AE, Attention_Wavenet\n",
    "\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "#     try:\n",
    "#         tf.config.experimental.set_virtual_device_configuration(\n",
    "#             gpus[0],\n",
    "#             [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#     except RuntimeError as e:\n",
    "#         # Virtual devices must be set before GPUs have been initialized\n",
    "#         print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=7)\n",
    "svm_clf = SVC(random_state=7, gamma='scale')\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=1, weights='distance', n_jobs=-1)\n",
    "mlp_clf = MLPClassifier(random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = './results/others/'\n",
    "inception = pd.read_csv(result_path + 'InceptionTime-128.csv')[['dataset_name', 'accuracy']]\n",
    "resnet_ucr = pd.read_csv(result_path + 'resnet-ucr.csv')[['dataset_name', 'accuracy']]\n",
    "resnet_uea = pd.read_csv(result_path + 'resnet-uea.csv')[['dataset_name', 'accuracy']]\n",
    "resnet_mts = pd.read_csv(result_path + 'resnet-mts.csv')[['dataset_name', 'accuracy']]\n",
    "hive_cote = pd.read_csv(result_path + 'singleTrainTest.csv')[['dataset_name', 'HIVE-COTE']]\n",
    "dtw_uea = pd.read_csv(result_path + 'usrl_uea.csv')[['dataset_name', 'DTW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def flatten_ts(train, test):\n",
    "    new_train, new_test = [], []\n",
    "    train_lens = []\n",
    "    \n",
    "    for _, row in train.iterrows():\n",
    "        for i in row.index:\n",
    "            train_lens.append(len(row[i]))\n",
    "\n",
    "    maxlen = np.ceil(np.average(train_lens)).astype(int)\n",
    "    \n",
    "    if maxlen >= 1500:\n",
    "        maxlen = 1250\n",
    "    \n",
    "    for _, row in train.iterrows():\n",
    "        new_list = []\n",
    "        for i in row.index:\n",
    "            ts = []\n",
    "            for j in range(len(row[i])):\n",
    "                ts.append(row[i][j])\n",
    "            new_list.append(ts)\n",
    "        new_train.append(pad_sequences(new_list, maxlen=maxlen, dtype='float32'))\n",
    "        \n",
    "    for _, row in test.iterrows():\n",
    "        new_list = []\n",
    "        for i in row.index:\n",
    "            ts = []\n",
    "            for j in range(len(row[i])):\n",
    "                ts.append(row[i][j])\n",
    "            new_list.append(ts)\n",
    "        new_test.append(pad_sequences(new_list, maxlen=maxlen, dtype='float32'))\n",
    "            \n",
    "    train_df = pd.DataFrame(np.array(new_train).reshape(train.shape[0], maxlen * train.columns.shape[0]))\n",
    "    test_df = pd.DataFrame(np.array(new_test).reshape(test.shape[0], maxlen * train.columns.shape[0]))\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(train_df)\n",
    "    return scaler.transform(train_df), scaler.transform(test_df), maxlen * train.columns.shape[0]\n",
    "#     return np.array(train_df), np.array(test_df), maxlen * train.columns.shape[0]\n",
    "\n",
    "def rnn_reshape(train, test, n_steps, n_features):\n",
    "#     train, test = flatten_ts(train, test)\n",
    "    return train.reshape(train.shape[0], n_steps, n_features), test.reshape(test.shape[0], n_steps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "# mc = keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TRepNet import TRepNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when tuning start with learning rate->mini_batch_size -> \n",
    "# momentum-> #hidden_units -> # learning_rate_decay -> #layers \n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def evaluate(data_name, univariate):\n",
    "    print('Data: ', data_name)\n",
    "    train_x, train_y, test_x, test_y = load_data(data_name, univariate=univariate)    \n",
    "#     n_steps = train_x.iloc[0][0].shape[0]\n",
    "    n_features = train_x.columns.shape[0]\n",
    "        \n",
    "    X_train, X_test, n_steps = flatten_ts(train_x, test_x)\n",
    "    X_train, X_test = rnn_reshape(X_train, X_test, n_steps // n_features, n_features)\n",
    "            \n",
    "    encoder, decoder = TRepNet(n_steps // n_features, n_features, activation='elu')\n",
    "    model = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "    plot_model(encoder, to_file='encoder.png', show_shapes=True, show_layer_names=True)\n",
    "    plot_model(decoder, to_file='decoder.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.compile(loss=\"mae\", optimizer=keras.optimizers.Nadam(lr=0.001, clipnorm=1.), metrics=['mae'])\n",
    "    history = model.fit(X_train, X_train, epochs=500, batch_size=16, validation_data=[X_test, X_test], callbacks=[es], verbose=0, shuffle=False)\n",
    "    \n",
    "    # Codings\n",
    "    codings_train = encoder.predict(X_train)\n",
    "    codings_test = encoder.predict(X_test)\n",
    "    \n",
    "#     # RF\n",
    "#     rf_clf.fit(codings_train, train_y)\n",
    "#     pred = rf_clf.predict(codings_test)\n",
    "#     rf_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "#     print('RF >>', rf_scores)\n",
    "\n",
    "    # SVM\n",
    "    svm_clf = SVC(random_state=7, gamma='scale')\n",
    "    nb_classes = np.unique(train_y).shape[0]\n",
    "    train_size = codings_train.shape[0]\n",
    "    if train_size // nb_classes < 5 or train_size < 50:\n",
    "        svm_clf.fit(codings_train, train_y)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(svm_clf, {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, np.inf]}, cv=5, iid=False, n_jobs=-1)\n",
    "        if train_size <= 10000:\n",
    "            grid_search.fit(codings_train, train_y)\n",
    "        else:\n",
    "            codings_train, _, train_y, _  = train_test_split(codings_train, train_y, train_size=10000, random_state=7, stratify=train_y)\n",
    "            grid_search.fit(codings_train, train_y)       \n",
    "        svm_clf = grid_search.best_estimator_\n",
    "\n",
    "        svm_clf.fit(codings_train, train_y)\n",
    "        \n",
    "#     svm_clf.fit(codings_train, train_y)\n",
    "    pred = svm_clf.predict(codings_test)\n",
    "    duration = time.time() - start_time\n",
    "    svm_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "    print('SVM >>', svm_scores)\n",
    "\n",
    "#     # 1-NN\n",
    "#     knn_clf.fit(codings_train, train_y)\n",
    "#     pred = knn_clf.predict(codings_test)\n",
    "#     knn_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "#     print('1-NN >>', knn_scores)\n",
    "\n",
    "#     # MLP\n",
    "#     mlp_clf.fit(codings_train, train_y)\n",
    "#     pred = mlp_clf.predict(codings_test)\n",
    "#     mlp_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "#     print('MLP >>', mlp_scores)\n",
    "\n",
    "    # SOTA Results\n",
    "    print('*'*10)\n",
    "    print('InceptionTime:', inception[inception['dataset_name'] == data_name]['accuracy'].values[0] if len(inception[inception['dataset_name'] == data_name]['accuracy'].values) >= 1 else 'N/A')\n",
    "    print('ResNet:', resnet_ucr[resnet_ucr['dataset_name'] == data_name]['accuracy'].values[0] if len(resnet_ucr[resnet_ucr['dataset_name'] == data_name]['accuracy'].values) >= 1 else 'N/A')\n",
    "    print('ResNet:', resnet_uea[resnet_uea['dataset_name'] == data_name]['accuracy'].values[0] if len(resnet_uea[resnet_uea['dataset_name'] == data_name]['accuracy'].values) >= 1 else 'N/A')\n",
    "    print('ResNet:', resnet_mts[resnet_mts['dataset_name'] == data_name]['accuracy'].values[0] if len(resnet_mts[resnet_mts['dataset_name'] == data_name]['accuracy'].values) >= 1 else 'N/A')\n",
    "    print('HIVE-COTE:', hive_cote[hive_cote['dataset_name'] == data_name]['HIVE-COTE'].values[0] if len(hive_cote[hive_cote['dataset_name'] == data_name]['HIVE-COTE'].values) >= 1 else 'N/A')\n",
    "    print('DTW:', dtw_uea[dtw_uea['dataset_name'] == data_name]['DTW'].values[0] if len(dtw_uea[dtw_uea['dataset_name'] == data_name]['DTW'].values) == 1 else 'N/A')\n",
    "    print('*'*10)\n",
    "    \n",
    "    results.append({'dataset': data_name, 'dim': codings_train.shape[1], \n",
    "#                     'RF-ACC': rf_scores['accuracy'], \n",
    "                    'SVM-ACC': svm_scores['accuracy'],\n",
    "#                     '1NN-ACC': knn_scores['accuracy'], \n",
    "                    # 'MLP-ACC': mlp_scores['accuracy'], \n",
    "#                     'RF-F1': rf_scores['f1'], \n",
    "                    'SVM-F1': svm_scores['f1'],\n",
    "#                     '1NN-F1': knn_scores['f1'], \n",
    "                    # 'MLP-F1': mlp_scores['f1'],\n",
    "                    'duration (sec)': duration\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mul_datasets = list(list_datasets()[1])\n",
    "# mul_datasets.remove('DuckDuckGeese')\n",
    "# mul_datasets.remove('EigenWorms')\n",
    "# mul_datasets.remove('FaceDetection')\n",
    "# mul_datasets.remove('Heartbeat')\n",
    "# mul_datasets.remove('InsectWingbeat')\n",
    "# mul_datasets.remove('LSST')\n",
    "# mul_datasets.remove('MotorImagery')\n",
    "# mul_datasets.remove('PEMS-SF')\n",
    "# StandWalkJump,'ERing', 'SpokenArabicDigits','BasicMotions', 'ArticularyWordRecognition', 'RacketSports', 'AtrialFibrillation', Cricket, 'PenDigits', 'Handwriting', \n",
    "\n",
    "selected_mul_datasets = ['EthanolConcentration', 'JapaneseVowels', 'SelfRegulationSCP1', 'HandMovementDirection', 'SelfRegulationSCP2']\n",
    "\n",
    "# uni_datasets = list(list_datasets()[0])\n",
    "# uni_datasets.remove('DodgerLoopDay')\n",
    "# uni_datasets.remove('DodgerLoopGame')\n",
    "# uni_datasets.remove('DodgerLoopWeekend')\n",
    "# uni_datasets.remove('ElectricDevices')\n",
    "# uni_datasets.remove('MelbournePedestrian')\n",
    "# uni_datasets.remove('PLAID')\n",
    "\n",
    "selected_uni_datasets = ['Earthquakes', 'ArrowHead', 'BeetleFly', 'ChlorineConcentration', 'Chinatown', 'DiatomSizeReduction', 'ECG200', 'ECG5000', 'ECGFiveDays',\n",
    "                         'FreezerSmallTrain', 'Fungi', 'GunPoint', 'GunPointAgeSpan','GunPointMaleVersusFemale', 'GunPointOldVersusYoung', 'Herring', \n",
    "                         'InsectEPGRegularTrain', 'InsectEPGSmallTrain', 'InsectWingbeatSound', 'Lightning2', 'MedicalImages', 'MiddlePhalanxTW',\n",
    "                         'NonInvasiveFetalECGThorax2', 'OliveOil', 'PhalangesOutlinesCorrect', 'PickupGestureWiimoteZ','PigAirwayPressure', 'PowerCons',\n",
    "                         'ProximalPhalanxOutlineAgeGroup', 'SemgHandGenderCh2', 'SemgHandMovementCh2', 'SemgHandSubjectCh2', 'SmoothSubspace', 'StarLightCurves',\n",
    "                         'SyntheticControl', 'Trace', 'UMD', 'UWaveGestureLibraryAll', 'Wafer', 'Yoga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Multivariate\n",
      "----------\n",
      "Data:  EthanolConcentration\n"
     ]
    }
   ],
   "source": [
    "# results = []\n",
    "\n",
    "# print('-'*10)\n",
    "# print('Univariate')\n",
    "# print('-'*10)\n",
    "\n",
    "# uni_datasets = ['Earthquakes', 'UMD', 'Wafer', 'GunPointOldVersusYoung', 'MiddlePhalanxTW', 'InsectWingbeatSound', 'OliveOil', 'NonInvasiveFetalECGThorax2', 'InsectEPGRegularTrain', 'UWaveGestureLibraryAll']\n",
    "# for dataset in selected_uni_datasets:\n",
    "#     evaluate(dataset, univariate=True)\n",
    "# pd.DataFrame(results).to_csv('./results/uni-TRepNet-results.csv', index=False)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "print('-'*10)\n",
    "print('Multivariate')\n",
    "print('-'*10)\n",
    "\n",
    "mul_datasets = ['BasicMotions', 'ERing', 'SpokenArabicDigits', 'AtrialFibrillation', 'EthanolConcentration']\n",
    "for dataset in selected_mul_datasets:\n",
    "    evaluate(dataset, univariate=False)\n",
    "print('='*10)\n",
    "pd.DataFrame(results).to_csv('./results/mul-TRepNet-results.csv', index=False)\n",
    "\n",
    "print('- END -')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
