{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_data, random_benchmark, list_datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from Imputation import remove_and_impute\n",
    "from Models import SAE, CNN_AE, LSTM_AE, GRU_AE, Bi_LSTM_AE, CNN_Bi_LSTM_AE, Causal_CNN_AE, Wavenet\n",
    "\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=7)\n",
    "svm_clf = SVC(random_state=7, gamma='scale')\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=1, weights='distance', n_jobs=-1)\n",
    "mlp_clf = MLPClassifier(random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TRepNet import TRepNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "# mc = keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def flatten_ts(train, test):\n",
    "    new_train, new_test = [], []\n",
    "    train_lens = []\n",
    "    \n",
    "    for _, row in train.iterrows():\n",
    "        for i in row.index:\n",
    "            train_lens.append(len(row[i]))\n",
    "\n",
    "    maxlen = np.ceil(np.average(train_lens)).astype(int)\n",
    "    \n",
    "    for _, row in train.iterrows():\n",
    "        new_list = []\n",
    "        for i in row.index:\n",
    "            ts = []\n",
    "            for j in range(len(row[i])):\n",
    "                ts.append(row[i][j])\n",
    "            new_list.append(ts)\n",
    "        new_train.append(pad_sequences(new_list, maxlen=maxlen, dtype='float32'))\n",
    "        \n",
    "    for _, row in test.iterrows():\n",
    "        new_list = []\n",
    "        for i in row.index:\n",
    "            ts = []\n",
    "            for j in range(len(row[i])):\n",
    "                ts.append(row[i][j])\n",
    "            new_list.append(ts)\n",
    "        new_test.append(pad_sequences(new_list, maxlen=maxlen, dtype='float32'))\n",
    "            \n",
    "    train_df = pd.DataFrame(np.array(new_train).reshape(train.shape[0], maxlen * train.columns.shape[0]))\n",
    "    test_df = pd.DataFrame(np.array(new_test).reshape(test.shape[0], maxlen * train.columns.shape[0]))\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(train_df)\n",
    "    return scaler.transform(train_df), scaler.transform(test_df), maxlen * train.columns.shape[0]\n",
    "#     return np.array(train_df), np.array(test_df), maxlen * train.columns.shape[0]\n",
    "\n",
    "def rnn_reshape(train, test, n_steps, n_features):\n",
    "#     train, test = flatten_ts(train, test)\n",
    "    return train.reshape(train.shape[0], n_steps, n_features), test.reshape(test.shape[0], n_steps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when tuning start with learning rate->mini_batch_size -> \n",
    "# momentum-> #hidden_units -> # learning_rate_decay -> #layers \n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def evaluate(fn, data_name, univariate):\n",
    "    print('Data: ', data_name)\n",
    "    train_x, train_y, test_x, test_y = load_data(data_name, univariate=univariate)    \n",
    "#     n_steps = train_x.iloc[0][0].shape[0]\n",
    "    n_features = train_x.columns.shape[0]\n",
    "        \n",
    "    X_train, X_test, n_steps = flatten_ts(train_x, test_x)\n",
    "    X_train, X_test = rnn_reshape(X_train, X_test, n_steps // n_features, n_features)\n",
    "            \n",
    "    encoder, decoder = fn(n_steps // n_features, n_features, activation='elu')\n",
    "    model = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "    model.compile(loss=\"mae\", optimizer=keras.optimizers.Nadam(lr=0.001, clipnorm=1.), metrics=['mae'])\n",
    "    history = model.fit(X_train, X_train, epochs=500, batch_size=16, validation_data=[X_test, X_test], callbacks=[es], verbose=0, shuffle=False)\n",
    "    \n",
    "    # Codings\n",
    "    codings_train = encoder.predict(X_train)\n",
    "    codings_test = encoder.predict(X_test)\n",
    "    \n",
    "    # RF\n",
    "    rf_clf.fit(codings_train, train_y)\n",
    "    pred = rf_clf.predict(codings_test)\n",
    "    rf_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "    print('RF >>', rf_scores)\n",
    "\n",
    "    # SVM\n",
    "    svm_clf.fit(codings_train, train_y)\n",
    "    pred = svm_clf.predict(codings_test)\n",
    "    svm_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "    print('SVM >>', svm_scores)\n",
    "\n",
    "    # 1-NN\n",
    "    knn_clf.fit(codings_train, train_y)\n",
    "    pred = knn_clf.predict(codings_test)\n",
    "    knn_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "    print('1-NN >>', knn_scores)\n",
    "\n",
    "    # MLP\n",
    "    mlp_clf.fit(codings_train, train_y)\n",
    "    pred = mlp_clf.predict(codings_test)\n",
    "    mlp_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "    print('MLP >>', mlp_scores)\n",
    "    \n",
    "    results.append({'dataset': data_name, 'dim': codings_train.shape[1], \n",
    "                    'RF-ACC': rf_scores['accuracy'], \n",
    "                    'SVM-ACC': svm_scores['accuracy'],\n",
    "                    '1NN-ACC': knn_scores['accuracy'], \n",
    "                    'MLP-ACC': mlp_scores['accuracy'], \n",
    "                    'RF-F1': rf_scores['f1'], \n",
    "                    'SVM-F1': svm_scores['f1'],\n",
    "                    '1NN-F1': knn_scores['f1'], \n",
    "                    'MLP-F1': mlp_scores['f1']\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  ArticularyWordRecognition\n",
      "RF >> {'accuracy': 0.9666666666666667, 'f1': 0.9661706235793192}\n",
      "SVM >> {'accuracy': 0.98, 'f1': 0.9798129117259553}\n",
      "1-NN >> {'accuracy': 0.9733333333333334, 'f1': 0.9735091787439615}\n",
      "MLP >> {'accuracy': 0.9733333333333334, 'f1': 0.9731185973446842}\n",
      "Data:  AtrialFibrillation\n",
      "RF >> {'accuracy': 0.26666666666666666, 'f1': 0.24444444444444446}\n",
      "SVM >> {'accuracy': 0.26666666666666666, 'f1': 0.23589743589743595}\n",
      "1-NN >> {'accuracy': 0.4666666666666667, 'f1': 0.4594017094017094}\n",
      "MLP >> {'accuracy': 0.3333333333333333, 'f1': 0.3277777777777778}\n",
      "Data:  BasicMotions\n",
      "RF >> {'accuracy': 0.95, 'f1': 0.949937343358396}\n",
      "SVM >> {'accuracy': 1.0, 'f1': 1.0}\n",
      "1-NN >> {'accuracy': 0.9, 'f1': 0.8958333333333333}\n",
      "MLP >> {'accuracy': 1.0, 'f1': 1.0}\n",
      "Data:  ERing\n",
      "RF >> {'accuracy': 0.9148148148148149, 'f1': 0.9143588428729911}\n",
      "SVM >> {'accuracy': 0.9148148148148149, 'f1': 0.9135668954392593}\n",
      "1-NN >> {'accuracy': 0.8962962962962963, 'f1': 0.894732835677418}\n",
      "MLP >> {'accuracy': 0.9259259259259259, 'f1': 0.9257400534399874}\n",
      "Data:  Handwriting\n",
      "RF >> {'accuracy': 0.2576470588235294, 'f1': 0.23096272303470858}\n",
      "SVM >> {'accuracy': 0.1776470588235294, 'f1': 0.11885135321708262}\n",
      "1-NN >> {'accuracy': 0.29058823529411765, 'f1': 0.27712448034571713}\n",
      "MLP >> {'accuracy': 0.2823529411764706, 'f1': 0.2617677130369561}\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "selected_mul_datasets = ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'Cricket',\n",
    "                         'ERing', 'HandMovementDirection', 'Handwriting', 'JapaneseVowels', 'PenDigits', 'RacketSports', 'SelfRegulationSCP1',\n",
    "                         'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'EthanolConcentration']\n",
    "\n",
    "for fn in [TRepNet]:\n",
    "    results = []\n",
    "    for dataset in ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'ERing', 'Handwriting']:\n",
    "        evaluate(fn, dataset, univariate=False)\n",
    "    pd.DataFrame(results).to_csv('./results/mul-'+ fn.__name__ +'-all-results.csv', index=False)\n",
    "print('END')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
