{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_data, random_benchmark, list_datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from Imputation import remove_and_impute\n",
    "from Models import SAE, CNN_AE, LSTM_AE, GRU_AE, Bi_LSTM_AE, CNN_Bi_LSTM_AE, Causal_CNN_AE, Wavenet, Attention_Bi_LSTM_AE, Attention_CNN_Bi_LSTM_AE, Attention_Wavenet\n",
    "\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_jobs=-1, random_state=7)\n",
    "svm_clf = SVC(gamma='scale', random_state=7)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=1, weights='distance', n_jobs=-1)\n",
    "mlp_clf = MLPClassifier(random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3072)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def flatten_ts(train, test):\n",
    "    new_train, new_test = [], []\n",
    "    train_lens = []\n",
    "    \n",
    "    for _, row in train.iterrows():\n",
    "        for i in row.index:\n",
    "            train_lens.append(len(row[i]))\n",
    "\n",
    "    maxlen = np.ceil(np.average(train_lens)).astype(int)\n",
    "    \n",
    "    for _, row in train.iterrows():\n",
    "        new_list = []\n",
    "        for i in row.index:\n",
    "            ts = []\n",
    "            for j in range(len(row[i])):\n",
    "                ts.append(row[i][j])\n",
    "            new_list.append(ts)\n",
    "        new_train.append(pad_sequences(new_list, maxlen=maxlen, dtype='float32'))\n",
    "        \n",
    "    for _, row in test.iterrows():\n",
    "        new_list = []\n",
    "        for i in row.index:\n",
    "            ts = []\n",
    "            for j in range(len(row[i])):\n",
    "                ts.append(row[i][j])\n",
    "            new_list.append(ts)\n",
    "        new_test.append(pad_sequences(new_list, maxlen=maxlen, dtype='float32'))\n",
    "            \n",
    "    train_df = pd.DataFrame(np.array(new_train).reshape(train.shape[0], maxlen * train.columns.shape[0]))\n",
    "    test_df = pd.DataFrame(np.array(new_test).reshape(test.shape[0], maxlen * train.columns.shape[0]))\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(train_df)\n",
    "    return scaler.transform(train_df), scaler.transform(test_df), maxlen * train.columns.shape[0]\n",
    "#     return np.array(train_df), np.array(test_df), maxlen * train.columns.shape[0]\n",
    "\n",
    "def rnn_reshape(train, test, n_steps, n_features):\n",
    "#     train, test = flatten_ts(train, test)\n",
    "    return train.reshape(train.shape[0], n_steps, n_features), test.reshape(test.shape[0], n_steps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "es = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model(n_steps, n_features, n_classes):\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.LSTM(128, return_sequences=True, input_shape=[n_steps, n_features]),\n",
    "        keras.layers.LSTM(128),\n",
    "        keras.layers.Dense(n_classes, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TRepNet import TRepNet\n",
    "# SyntheticControl, PhalangesOutlinesCorrect\n",
    "# SelfRegulationSCP2, SelfRegulationSCP1\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  TwoPatterns\n"
     ]
    }
   ],
   "source": [
    "data_name = 'TwoPatterns'\n",
    "print('Data: ', data_name)\n",
    "train_x, train_y, test_x, test_y = load_data(data_name, univariate=True)\n",
    "\n",
    "n_features = train_x.columns.shape[0]\n",
    "\n",
    "X_train, X_test, n_steps = flatten_ts(train_x, test_x)\n",
    "X_train, X_test = rnn_reshape(X_train, X_test, n_steps // n_features, n_features)\n",
    "\n",
    "encoder, decoder = TRepNet(n_steps // n_features, n_features, activation='elu')\n",
    "model = keras.models.Sequential([encoder, decoder])\n",
    "model.compile(loss=\"mae\", optimizer=keras.optimizers.Nadam(lr=0.001, clipnorm=1.), metrics=['mae'])\n",
    "history = model.fit(X_train, X_train, epochs=500, batch_size=16, validation_data=[X_test, X_test], callbacks=[es], verbose=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_name, univariate, lb_rate):\n",
    "    train_x, train_y, test_x, test_y = load_data(data_name, univariate=univariate)    \n",
    "    n_features = train_x.columns.shape[0]\n",
    "\n",
    "    X_train, X_test, n_steps = flatten_ts(train_x, test_x)\n",
    "    X_train, X_test = rnn_reshape(X_train, X_test, n_steps // n_features, n_features)\n",
    "    if lb_rate > np.unique(train_y).shape[0] and train_x.shape[0] - lb_rate > np.unique(train_y).shape[0]:\n",
    "        X_train, X_test, n_steps = flatten_ts(train_x, test_x)\n",
    "        X_train, _, train_y, _ = train_test_split(X_train, train_y, train_size=lb_rate, stratify=train_y, random_state=7)\n",
    "        \n",
    "#         train_y = np.where(train_y == 'left', 0, train_y)\n",
    "#         train_y = np.where(train_y == 'right', 1, train_y)\n",
    "#         test_y = np.where(test_y == 'left', 0, test_y)\n",
    "#         test_y = np.where(test_y == 'right', 1, test_y)\n",
    "        \n",
    "        # # RF\n",
    "        # rf_clf.fit(X_train, train_y)\n",
    "        # pred = rf_clf.predict(X_test)\n",
    "        # rf_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "        # print('RF >>', rf_scores)\n",
    "\n",
    "        # SVM\n",
    "        svm_clf = SVC(gamma='scale', random_state=7)\n",
    "        svm_clf.fit(X_train, train_y)\n",
    "        pred = svm_clf.predict(X_test)\n",
    "        svm_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "        print('B-SVM >>', svm_scores)\n",
    "\n",
    "        # # 1-NN\n",
    "        # knn_clf.fit(X_train, train_y)\n",
    "        # pred = knn_clf.predict(X_test)\n",
    "        # knn_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "        # print('1-NN >>', knn_scores)\n",
    "\n",
    "        # # MLP\n",
    "        # mlp_clf.fit(X_train, train_y)\n",
    "        # pred = mlp_clf.predict(X_test)\n",
    "        # mlp_scores = {'accuracy': accuracy_score(test_y, pred), 'f1': f1_score(test_y, pred, average='weighted')}\n",
    "        # print('MLP >>', mlp_scores)\n",
    "\n",
    "        # LSTM\n",
    "        X_train, X_test = rnn_reshape(X_train, X_test, n_steps // n_features, n_features)\n",
    "#         train_y, test_y = train_y.astype(float), test_y.astype(float)\n",
    "        n_classes = np.unique(train_y).shape[0]\n",
    "        if np.min(train_y.astype(int)) == 1:\n",
    "            n_classes = n_classes + 1\n",
    "        y_train, y_test = keras.utils.to_categorical(train_y), keras.utils.to_categorical(test_y)\n",
    "        model = LSTM_Model(n_steps // n_features, n_features, n_classes)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        history = model.fit(X_train, y_train, epochs=10, validation_split=0.1, shuffle=False, verbose=0, callbacks=[es])\n",
    "#         pred = keras.utils.to_categorical(model.predict_classes(X_test))\n",
    "        lstm_scores = {'accuracy': model.evaluate(X_test, y_test, verbose=0)[1], 'f1': ''}\n",
    "        print('LSTM >>', lstm_scores)\n",
    "        \n",
    "        # TRepNet-SVM\n",
    "#         X_train, X_test = rnn_reshape(X_train, X_test, n_steps // n_features, n_features)\n",
    "        # Codings\n",
    "        codings_train = encoder.predict(X_train)\n",
    "        codings_test = encoder.predict(X_test)\n",
    "        \n",
    "        tsvm_clf = SVC(random_state=7, gamma='scale')\n",
    "        nb_classes = np.unique(train_y).shape[0]\n",
    "        train_size = codings_train.shape[0]\n",
    "        if train_size // nb_classes < 5 or train_size < 50:\n",
    "            tsvm_clf.fit(codings_train, train_y)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(svm_clf, {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, np.inf]}, cv=5, iid=False, n_jobs=-1)\n",
    "            if train_size <= 10000:\n",
    "                grid_search.fit(codings_train, train_y)\n",
    "            else:\n",
    "                codings_train, _, train_y, _  = train_test_split(codings_train, train_y, train_size=10000, random_state=7, stratify=train_y)\n",
    "                grid_search.fit(codings_train, train_y)       \n",
    "            tsvm_clf = grid_search.best_estimator_\n",
    "\n",
    "            tsvm_clf.fit(codings_train, train_y)\n",
    "\n",
    "        tpred = tsvm_clf.predict(codings_test)\n",
    "        TrepNet_scores = {'accuracy': accuracy_score(test_y, tpred), 'f1': f1_score(test_y, tpred, average='weighted')}\n",
    "        print('TRepNet-SVM >>', TrepNet_scores)\n",
    "        \n",
    "        results.append({'dataset': data_name, 'dim': str(n_steps)+', '+str(n_features), '# Labels': lb_rate,\n",
    "                        # 'RF-ACC': rf_scores['accuracy'], \n",
    "                        'SVM-ACC': svm_scores['accuracy'],\n",
    "                        # '1NN-ACC': knn_scores['accuracy'],\n",
    "                        # 'MLP-ACC': mlp_scores['accuracy'], \n",
    "                        'LSTM-ACC': lstm_scores['accuracy'],\n",
    "                        'TRepNet-ACC': TrepNet_scores['accuracy'],\n",
    "                        # 'RF-F1': rf_scores['f1'], \n",
    "                        'SVM-F1': svm_scores['f1'],\n",
    "                        # '1NN-F1': knn_scores['f1'], \n",
    "                        # 'MLP-F1': mlp_scores['f1'],\n",
    "                        'LSTM-F1': lstm_scores['f1'],\n",
    "                        'TRepNet-F1': TrepNet_scores['f1']\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label # 10\n",
      "B-SVM >> {'accuracy': 0.341, 'f1': 0.23254596532534247}\n",
      "LSTM >> {'accuracy': 0.459, 'f1': ''}\n",
      "TRepNet-SVM >> {'accuracy': 0.35575, 'f1': 0.2402914694937631}\n",
      "Label # 20\n",
      "B-SVM >> {'accuracy': 0.4195, 'f1': 0.4197986561940946}\n",
      "LSTM >> {'accuracy': 0.44125, 'f1': ''}\n",
      "TRepNet-SVM >> {'accuracy': 0.449, 'f1': 0.44742501055773143}\n",
      "Label # 30\n",
      "B-SVM >> {'accuracy': 0.4275, 'f1': 0.3953407561540016}\n",
      "LSTM >> {'accuracy': 0.47925, 'f1': ''}\n",
      "TRepNet-SVM >> {'accuracy': 0.4025, 'f1': 0.3409962300872288}\n",
      "Label # 40\n",
      "B-SVM >> {'accuracy': 0.42275, 'f1': 0.3835733413994904}\n",
      "LSTM >> {'accuracy': 0.4885, 'f1': ''}\n",
      "TRepNet-SVM >> {'accuracy': 0.443, 'f1': 0.39578424254094474}\n",
      "Label # 50\n",
      "B-SVM >> {'accuracy': 0.4755, 'f1': 0.4564317458700733}\n",
      "LSTM >> {'accuracy': 0.49675, 'f1': ''}\n",
      "TRepNet-SVM >> {'accuracy': 0.533, 'f1': 0.5293335928019307}\n",
      "Label # 100\n",
      "B-SVM >> {'accuracy': 0.5285, 'f1': 0.5194589636698052}\n",
      "LSTM >> {'accuracy': 0.536, 'f1': ''}\n",
      "TRepNet-SVM >> {'accuracy': 0.5875, 'f1': 0.5845055764999463}\n",
      "Label # 150\n",
      "B-SVM >> {'accuracy': 0.595, 'f1': 0.5936606562898534}\n",
      "LSTM >> {'accuracy': 0.55325, 'f1': ''}\n",
      "TRepNet-SVM >> {'accuracy': 0.656, 'f1': 0.6557447746946683}\n",
      "Label # 200\n",
      "B-SVM >> {'accuracy': 0.64525, 'f1': 0.6453228784505809}\n",
      "LSTM >> {'accuracy': 0.625, 'f1': ''}\n",
      "TRepNet-SVM >> {'accuracy': 0.72125, 'f1': 0.721005268161884}\n",
      "Label # 300\n",
      "B-SVM >> {'accuracy': 0.67475, 'f1': 0.6736390305410982}\n",
      "LSTM >> {'accuracy': 0.67675, 'f1': ''}\n",
      "TRepNet-SVM >> {'accuracy': 0.70875, 'f1': 0.7077044681446535}\n"
     ]
    }
   ],
   "source": [
    "# selected_uni_datasets = ['ArrowHead', 'BeetleFly', 'ChlorineConcentration', 'Crop', 'Earthquakes','ECG200', 'ECG5000', 'ECGFiveDays',\n",
    "#                          'FreezerSmallTrain', 'Fungi', 'GunPoint', 'GunPointAgeSpan','GunPointMaleVersusFemale', 'GunPointOldVersusYoung', 'Herring', \n",
    "#                          'InsectEPGRegularTrain', 'InsectEPGSmallTrain', 'InsectWingbeatSound', 'Lightning2', 'MedicalImages', 'MiddlePhalanxTW',\n",
    "#                          'NonInvasiveFetalECGThorax2', 'OliveOil', 'PhalangesOutlinesCorrect', 'PickupGestureWiimoteZ','PigAirwayPressure', 'PowerCons',\n",
    "#                          'ProximalPhalanxOutlineAgeGroup', 'SemgHandGenderCh2', 'SemgHandMovementCh2', 'SemgHandSubjectCh2', 'SmoothSubspace', 'StarLightCurves',\n",
    "#                          'SyntheticControl', 'Trace', 'UMD', 'UWaveGestureLibraryAll', 'Wafer', 'Yoga'] # DiatomSizeReduction\n",
    "\n",
    "# for mr in [int(10 * pow(1.5, i)) for i in range(11)]:\n",
    "#     print('Label #', mr)\n",
    "#     results = []\n",
    "#     for dataset in selected_uni_datasets:\n",
    "#         evaluate(dataset, univariate=True, lb_rate=mr)\n",
    "#     pd.DataFrame(results).to_csv('./results/sparse labels/uni-LSTM-' + str(mr) +'.csv', index=False)\n",
    "\n",
    "results = []\n",
    "for mr in [10, 20, 30, 40, 50, 100, 150, 200, 300]:\n",
    "    print('Label #', mr)\n",
    "    evaluate(data_name, univariate=True, lb_rate=mr)\n",
    "pd.DataFrame(results).to_csv('./results/sparse labels/uni-'+data_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_mul_datasets = ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'Cricket', 'EthanolConcentration',\n",
    "#                          'ERing', 'HandMovementDirection', 'Handwriting', 'JapaneseVowels', 'PenDigits', 'RacketSports', 'SelfRegulationSCP1',\n",
    "#                          'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump']\n",
    "                         \n",
    "# for mr in [int(10 * pow(1.5, i)) for i in range(11)]:\n",
    "#     print('Label #', mr)\n",
    "#     results = []\n",
    "#     for dataset in [selected_mul_datasets]:\n",
    "#         evaluate(dataset, univariate=False, lb_rate=mr)\n",
    "#     pd.DataFrame(results).to_csv('./results/sparse labels/mul-LSTM-' + str(mr) + '.csv', index=False)\n",
    "\n",
    "# results = []\n",
    "# for mr in [20, 30, 40, 50, 100, 150, 200, 300]:\n",
    "#     print('Label #', mr)\n",
    "#     evaluate(data_name, univariate=False, lb_rate=mr)\n",
    "# pd.DataFrame(results).to_csv('./results/sparse labels/mul-'+data_name+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
